grad_at_reg_mode_from_regular <- estimate_grad(post_mode_regular_a$par,info_regular)
grad_at_reg_mode_from_weak <- estimate_grad(post_mode_regular_b$par,info_weak)
grad_at_reg_mode_from_regular
grad_at_reg_mode_from_weak
grad_at_reg_mode_from_regular <- estimate_grad(post_mode_weak$par,info_regular)
grad_at_reg_mode_from_weak <- estimate_grad(post_mode_weak$par,info_weak)
grad_at_reg_mode_from_regular
grad_at_reg_mode_from_weak
g1 <- t(info_regular$design_matrix) %*% (info_regular$yobs - boot::inv.logit(0))
g1
g2<- t(info_weak$design_matrix) %*% (info_weak$yobs - boot::inv.logit(0))
g2
View(info_weak)
View(info_weak)
grad_at_reg_mode_from_regular <- estimate_grad(post_mode_weak$par,info_regular)
grad_at_reg_mode_from_weak <- estimate_grad(post_mode_weak$par,info_weak)
grad_at_reg_mode_from_regular
grad_at_reg_mode_from_weak
grad_at_reg_mode_from_regular <- estimate_grad(post_mode_regular_a$par,info_regular)
grad_at_reg_mode_from_weak <- estimate_grad(post_mode_regular_a$par,info_weak)
grad_at_reg_mode_from_regular
grad_at_reg_mode_from_weak
grad_at_reg_mode_from_regular <- estimate_grad(post_mode_weak$par,info_regular)
grad_at_reg_mode_from_weak <- estimate_grad(post_mode_weak$par,info_weak)
grad_at_reg_mode_from_regular
grad_at_reg_mode_from_weak
grad_at_mle_from_regular <- estimate_grad(coef(mod35),info_regular)
grad_at_mle_from_weak <- estimate_grad(coef(mod35),info_weak)
grad_at_mle_from_regular
grad_at_mle_from_weak
coef(mod35)
mod35
###
mod35 <- glm(y ~ x, family = binomial(), data = df01)
coef(mod35)
grad_at_mle_from_regular <- estimate_grad(coef(mod35),info_regular)
grad_at_mle_from_weak <- estimate_grad(coef(mod35),info_weak)
grad_at_mle_from_regular
grad_at_mle_from_weak
grad_logpost <- function(unknowns, my_info)
{
# extract the design matrix and assign to X
X <- my_info$design_matrix
# include as many lines of code as you feel are necessary
# calculate the linear predictor
eta <- X %*% as.matrix(unknowns)
mu <- boot::inv.logit(eta)
G<-t(X) %*% (my_info$yobs - boot::inv.logit(0))
# return the gradient of the log-posterior
return G
grad_logpost <- function(unknowns, my_info)
{
# extract the design matrix and assign to X
X <- my_info$design_matrix
# include as many lines of code as you feel are necessary
# calculate the linear predictor
eta <- X %*% as.matrix(unknowns)
mu <- boot::inv.logit(eta)
G<-t(X) %*% (my_info$yobs - boot::inv.logit(0))
# return the gradient of the log-posterior
return(G)
}
grad_logpost(c(-1,-1),info_regular)
grad_logpost <- function(unknowns, my_info)
{
# extract the design matrix and assign to X
X <- my_info$design_matrix
# include as many lines of code as you feel are necessary
# calculate the linear predictor
eta <- X %*% as.matrix(unknowns)
mu <- boot::inv.logit(eta)
G<-t(X) %*% (my_info$yobs - boot::inv.logit(0))
# return the gradient of the log-posterior
return(G)
}
grad_logpost(c(-1,-1),info_regular)
logistic_logpost(c(-1,-1),info_regular)
grad_logpost <- function(unknowns, my_info)
{
# extract the design matrix and assign to X
X <- my_info$design_matrix
# include as many lines of code as you feel are necessary
# calculate the linear predictor
eta <- X %*% as.matrix(unknowns)
mu <- boot::inv.logit(eta)
G<-t(X) %*% (my_info$yobs - boot::inv.logit(0))
# return the gradient of the log-posterior
return(G)
}
grad_logpost(c(-1,-1),info_regular)
tem                       <- optim(c(0, 0),
logistic_logpost,
info_regular,
gr = NULL,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
tem
grad_logpost <- function(unknowns, my_info)
{
# extract the design matrix and assign to X
X <- my_info$design_matrix
# include as many lines of code as you feel are necessary
# calculate the linear predictor
eta <- X %*% as.matrix(unknowns)
mu <- boot::inv.logit(eta)
G<-t(X) %*% (my_info$yobs - boot::inv.logit(0))
# return the gradient of the log-posterior
return(G)
}
grad_logpost(c(-1,-1),info_regular)
estimate_grad(c(-1,-1),info_regular)
grad_logpost <- function(unknowns, my_info)
{
# extract the design matrix and assign to X
X <- my_info$design_matrix
# include as many lines of code as you feel are necessary
# calculate the linear predictor
eta <- X %*% as.matrix(unknowns)
mu <- boot::inv.logit(eta)
G<-t(X) %*% (my_info$yobs - boot::inv.logit(mu))
# return the gradient of the log-posterior
return(G)
}
grad_logpost(c(-1,-1),info_regular)
estimate_grad(c(-1,-1),info_regular)
grad_logpost <- function(unknowns, my_info)
{
# extract the design matrix and assign to X
X <- my_info$design_matrix
# include as many lines of code as you feel are necessary
# calculate the linear predictor
eta <- X %*% as.matrix(unknowns)
#mu <- boot::inv.logit(eta)
G<-t(X) %*% (my_info$yobs - boot::inv.logit(eta))
# return the gradient of the log-posterior
return(G)
}
grad_logpost(c(-1,-1),info_regular)
estimate_grad(c(-1,-1),info_regular)
grad_logpost <- function(unknowns, my_info)
{
# extract the design matrix and assign to X
X <- my_info$design_matrix
# include as many lines of code as you feel are necessary
# calculate the linear predictor
eta <- X %*% as.matrix(unknowns)
#mu <- boot::inv.logit(eta)
G<-t(X) %*% (my_info$yobs - boot::inv.logit(eta))
# return the gradient of the log-posterior
return(G)
}
grad_logpost(c(-1,-1),info_regular)
estimate_grad(c(-1,-1),info_regular)
grad_logpost(c(0,0),info_regular)
estimate_grad(c(0,0),info_regular)
grad_logpost(c(2,2),info_regular)
estimate_grad(c(2,2),info_regular)
post_mode_regular_with_grad_a <- optim(c(0, 0),
logistic_logpost,
info_regular,
gr = grad_logpost,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_regular_with_grad_b <- optim(c(2, 2),
logistic_logpost,
info_regular,
gr = grad_logpost,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_regular_with_grad_b <- optim(c(2, 2),
logistic_logpost,
info_regular,
gr = grad_logpost,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_regular_with_grad_a
post_mode_regular_with_grad_b
post_mode_regular_a <- optim(c(0, 0),
logistic_logpost,
info_regular,
gr = NULL,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_regular_a
post_mode_regular_b <- optim(c(2, 2),
logistic_logpost,
info_regular,
gr = NULL,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_regular_b
post_mode_weak_with_grad <-  optim(c(0, 0),
logistic_logpost,
info_weak,
gr = grad_logpost,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_weak_with_grad
train_small <- df01 %>% slice(1:5)
###
train_small%>%
ggplot()+
geom_point()+
geom_vline(xintercept = -0.6)
###
train_small%>%
ggplot(mapping = aes(x =x, y=y))+
geom_point()+
geom_vline(xintercept = -0.6)
info_small_regular <- list(
yobs = train_small$y ,
design_matrix = model.matrix( y ~  x,train_small ),
mu_beta =0 ,
tau_beta =2.5
)
info_small_weak <- list(
yobs =train_small$y ,
design_matrix =  model.matrix( y ~  x,train_small ),
mu_beta = 0,
tau_beta = 50
)
post_mode_reg_from_small <- optim(c(0, 0),
logistic_logpost,
info_small_regular,
gr = grad_logpost,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_weak_from_small <- optim(c(0,0),
logistic_logpost,
info_small_weak,
gr = grad_logpost,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_reg_from_small
post_mode_weak_from_small
post_mode_weak <- optim(c(0, 0),
logistic_logpost,
info_weak,
gr = NULL,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_weak
###
post_mode_reg_from_small$hessian
#post_corr_reg_from_small
#post_corr_weak_from_small
###
solve(post_mode_reg_from_small$hessian)
#post_corr_reg_from_small
#post_corr_weak_from_small
###
t(post_mode_reg_from_small$hessian)
#post_corr_reg_from_small
#post_corr_weak_from_small
###
(post_mode_reg_from_small$hessian)^(-1)
#post_corr_reg_from_small
#post_corr_weak_from_small
###
b<-c(-0.2096555,0.1215903)
b1<-c(0.12159028,-0.07654121)
#post_corr_reg_from_small
#post_corr_weak_from_small
###
b<-c(-0.2096555,0.1215903)
b1<-c(0.12159028,-0.07654121)
xmat<-cbind(b,b1)
xmat
#post_corr_reg_from_small
#post_corr_weak_from_small
###
b<-c(-0.2096555,0.1215903)
b1<-c(0.12159028,-0.07654121)
xmat<-as.matrix(cbind(b,b1))
xmat
#post_corr_reg_from_small
#post_corr_weak_from_small
###
b<-c(-0.2096555,0.1215903)
b1<-c(0.12159028,-0.07654121)
xmat<-as.matrix(cbind(b,b1))
(xmat)^(-1)
#post_corr_reg_from_small
#post_corr_weak_from_small
###
b<-c(-0.2096555,0.1215903)
b1<-c(0.12159028,-0.07654121)
xmat<-as.matrix(cbind(b,b1))
solve(xmat)
#post_corr_reg_from_small
#post_corr_weak_from_small
###
b<-c(-0.2096555,0.1215903)
b1<-c(0.12159028,-0.07654121)
xmat<-as.matrix(cbind(b,b1))
-solve(xmat)
#post_corr_reg_from_small
#post_corr_weak_from_small
###
b<-c(-0.2096555,0.1215903)
b1<-c(0.12159028,-0.07654121)
xmat<-as.matrix(cbind(b,b1))
qq<-(-solve(xmat))
cov2cor(qq)
#post_corr_reg_from_small
#post_corr_weak_from_small
###
post_corr_reg_from_small<- cov2cor(-solve(post_mode_reg_from_small$hessian))
post_corr_reg_from_small
post_corr_weak_from_small<- cov2cor(-solve(post_mode_weak_from_small$hessian))
post_corr_reg_from_small
###
mod_05 <- glm(y ~ x, family = binomial(), data = train_small)
coef(mod_05)
###
mod_05 <- glm(y ~ x, family = binomial(), data = train_small)
coef(mod_05)
###
mod_05 <- glm(y ~ x, family = binomial(), data = train_small)
mod_05
coef(mod_05)
###
mod_05 <- glm(y ~ x, family = binomial(), data = train_small)
mod_05
knitr::opts_chunk$set(echo = TRUE)
###
mod_05 <- glm(y ~ x, family = binomial(), data = train_small)
mod_05
knitr::opts_chunk$set(echo = TRUE)
post_mode_weak <- optim(c(0, 0),
logistic_logpost,
info_weak,
gr = NULL,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_weak
post_mode_regular_a <- optim(c(0, 0),
logistic_logpost,
info_regular,
gr = NULL,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_regular_a
post_mode_regular_b <- optim(c(2, 2),
logistic_logpost,
info_regular,
gr = NULL,
method = "BFGS",
hessian = TRUE,
control = list(fnscale = -1, maxit = 1001))
post_mode_regular_b
###
post_corr_reg_from_small<- cov2cor(-solve(post_mode_reg_from_small$hessian))
post_corr_reg_from_small
post_corr_weak_from_small<- cov2cor(-solve(post_mode_weak_from_small$hessian))
post_corr_weak_from_small
knitr::opts_chunk$set(echo = TRUE)
grad_at_0_from_regular <- estimate_grad (c(0,0),info_regular)
grad_at_0_from_weak <- estimate_grad (c(0,0),info_weak)
grad_at_0_from_regular
grad_at_0_from_weak
knitr::opts_chunk$set(echo = TRUE)
set.seed(4321)
fit_glmnet_sonar <- train(Class ~ ., data = Sonar,
method = "glmnet",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE )
library(caret)
set.seed(4321)
fit_glmnet_sonar <- train(Class ~ ., data = Sonar,
method = "glmnet",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE )
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
data("Sonar", package = 'mlbench')
Sonar %>% dim()
Sonar %>%
tibble::rowid_to_column("obs_id") %>%
tidyr::gather(key = "input_name",
value = "input_value",
-obs_id, -Class) %>%
mutate(input_id = stringr::str_extract(input_name, "\\d+")) %>%
mutate_at("input_id", as.numeric) %>%
tibble::as_tibble() %>%
ggplot(mapping = aes(x = input_id,
y = input_value)) +
geom_boxplot(mapping = aes(group = input_id)) +
theme_bw()
Sonar%>%
dplyr::select(-Class)%>%
cor()%>%
corrplot::corrplot(method = "square",order = "hclust")
Sonar%>%
ggplot(aes(x = Class))+
geom_bar()
ctrl_k05_roc <- trainControl(method = "cv", number = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = TRUE)
metric_sonar <- ctrl_k05_roc
set.seed(4321)
fit_glm_sonar <- train(Class ~ ., data = Sonar,
method = "glm",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE )
fit_glm_sonar
set.seed(4321)
fit_glmnet_sonar <- train(Class ~ ., data = Sonar,
method = "glmnet",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE )
fit_glmnet_sonar
set.seed(4321)
fit_glmnet_sonar <- train(Class ~ ., data = Sonar,
method = "glmnet",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE )
fit_glmnet_sonar$results
set.seed(4321)
fit_glmnet_sonar <- train(Class ~ ., data = Sonar,
method = "glmnet",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE )
fit_glmnet_sonar$pred
set.seed(4321)
fit_glmnet_sonar <- train(Class ~ ., data = Sonar,
method = "glmnet",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE )
fit_glmnet_sonar$modelInfo
set.seed(4321)
fit_glmnet_sonar <- train(Class ~ ., data = Sonar,
method = "glmnet",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE )
fit_glmnet_sona
set.seed(4321)
fit_glmnet_sonar <- train(Class ~ ., data = Sonar,
method = "glmnet",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE )
fit_glmnet_sona
set.seed(4321)
fit_glmnet_sonar <- train(Class ~ ., data = Sonar,
method = "glmnet",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE )
fit_glmnet_sonar
set.seed(4321)
fit_nnet_sonar <- train(Class ~ ., data = Sonar,
method = "nnet",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE)
fit_nnet_sonar
set.seed(4321)
fit_rf_sonar <- train(Class ~ ., data = Sonar,
method = "rf",
metric = "ROC",
preProcess = c("center", "scale"),
trControl = metric_sonar,
trace=FALSE,
importance = TRUE)
fit_rf_sonar
set.seed(4321)
fit_xgb_sonar <- train(Class ~ ., data = Sonar,
method = "xgbTree",
metric = "ROC",
trControl = metric_sonar,
importance=TRUE)
fit_xgb_sonar$bestTune
plot(fit_xgb_sonar)
